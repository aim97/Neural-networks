{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tensorflow\n",
    "\n",
    "## content\n",
    "1. Tensorflow 1 workflow\n",
    "2. Tensorflow 2 what's new\n",
    "3. Model training in tensorflow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorflow1 Workflow\n",
    "![Tensorflow-work-flow](../images/Tensorflow-work-flow.png)  \n",
    "\n",
    "Tensorflow is based on a **computation graph**, you first have to specify how the model will work through the computation graph, this part is just **DECLARATIVE**, you don't do any compuations here, you simply specify how the computations will be done.  \n",
    "\n",
    "Then you need to specify the *hyper parameters* like learning rate, *loss function*, *optimizer*, ... etc, all of this affect the results.  \n",
    "\n",
    "Now we have a clear idea about how the computation will occur in the model and how the model should be trained we have to start a ***Session***, whatever work you want tensorflow to do, it has to be done within the scope of a session.  \n",
    "\n",
    "Remember that we said the computation graph is a declarative process, so in order to work with what we declared we have to actually initialize it, and allocate memory for it first.  \n",
    "\n",
    "**Training the model**: this what we actually use tensorflow for, given the computation graph you provided tensorflow randomly initializes the weights, runs the graph, and then optimizes the weights.  \n",
    "\n",
    "This is the overall view of how **Tensorflow** works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2 What's new\n",
    "This is all from the past, the past were to tensorflow was really hard to code and work with, so what have changed?\n",
    "\n",
    "- **No more Sessions and welcome Eager execution**: rather than cutting the work flow into declarative part to define the graph and execution part within the session, now *tensorflow code is like any other code*, you write your code and execute it in the spot.\n",
    "- **Keras is now more implemented inside tensorflow** and become more scalable: estimators were removed and now your work will scale on its own.\n",
    "- **API clean up (strctured update)**: changes in function names, modules, most of v1.x stuff were moved under ***tf.compat.v1. ...*** you can use it from there, except the contrib part which was removed for good.\n",
    "- **No more globals**: if you lose a variable it gets garbage collected.\n",
    "\n",
    "you can find more by watching [this](https://www.youtube.com/watch?v=k5c-vg4rjBw) or reading [that](https://www.tensorflow.org/guide/effective_tf2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples on Tensorflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(4)\n",
    "b = tf.constant(5)\n",
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training in Tensorflow 2\n",
    "it's very similar to the diagram above except with no sessions, you first define the model, *compile it*, train it, then run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import the needed layers\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential model\n",
    "\n",
    "First we need to create a sequential model, this is the most basic model in tensorflow, it's a linear stack of layers, you can think of it as a stack of layers, each layer is a function that takes the output of the previous layer as input and returns the output of the next layer.\n",
    "Later we will see more complex models where we will use the tensorflow [functional API](https://www.tensorflow.org/guide/keras/functional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# this is the simplest model, consisiting of a single neuron, with single input and linear activation\n",
    "model = tf.keras.Sequential([\n",
    "  Dense(units=1, input_shape=[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compilation\n",
    "\n",
    "In this part we will see how to compile a model, this is the first step of training a model, we use it to specify *loss function*, *optimizer*, *metrics* and *optimizer* to the model, youn can check the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) for more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# let's learn some linear function y = 5 * x + 10\n",
    "import numpy as np\n",
    "x = np.arange(0, 10, 0.1)\n",
    "y = 5 * x + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "In this part we will see how to train a model, we will use the [Keras API](https://www.tensorflow.org/guide/keras/overview) to train a model, we need to pass the data to train on, as well as some training parameters like `batch_size`, `epochs` ...etc, you can check the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 743.6291\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 19.1568\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 18.7858\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.3673\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 17.2387\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.2360\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 15.9266\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 15.4453\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 14.8113\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 14.3917\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 13.7351\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 13.5082\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 12.9473\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 12.2818\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 907us/step - loss: 11.7343\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 11.4351\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 12.2326\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 10.8037\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 10.1819\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 9.8293\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 9.4056\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 9.0374\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.5833\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.2094\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 7.9820\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 7.6595\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 7.2112\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.9919\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.6864\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.4806\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 6.2242\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.0014\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.8358\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 5.4847\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 5.2259\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1130\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.8464\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.7973\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.5119\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 4.5304\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.1678\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.9974\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.8503\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 3.6917\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5521\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5119\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 3.7474\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 3.2138\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 3.1084\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.9320\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.8148\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.0109\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.6116\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.4953\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 2.4258\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 2.3728\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 2.2510\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.1526\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 2.1886\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.0241\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.9936\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.9021\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 1.7880\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 1.6930\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.7135\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5847\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 1.5182\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 1.5156\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5281\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 1.3796\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 666us/step - loss: 1.3192\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2639\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2265\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 998us/step - loss: 1.1552\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 1.1114\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0676\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0300\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.0226\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.9906\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9262\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9701\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.8577\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.8259\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.7894\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7750\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7372\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7021\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.6785\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.6571\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6145\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 667us/step - loss: 0.5955\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 999us/step - loss: 0.6092\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5753\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5173\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4962\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4831\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.4601\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4484\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 1000us/step - loss: 0.4165\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4228\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, y, 'go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results\n",
    "\n",
    "You can access the results of the training by using the `history` attribute of the model, it's a dictionary with all the training metrics, returned by the `fit` function after the model is trained, you can check the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) for more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], 'r-')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70d077a54fc0094e614fa5b0f03887f72b81a6c521e79d0a3e89b1c317604ecf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
