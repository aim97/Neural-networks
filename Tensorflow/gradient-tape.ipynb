{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gradient Tape\n",
    "\n",
    "As the name suggests, it's a *tape* to store the *gradients* of *watched values*.\n",
    "\n",
    "As you should know by now, learning is the process of finding the best weights to minimize the loss function, and we use the optimizer to update the weights to achieve that. optimizers usually use gradients (derivative of the loss function w.r.t the weights) to update the weights in the best direction. in order to do that we need alot of intermediate values computed through the forward pass to be stored, we also need to remember the functions used to compute them to calculate their derivatives.\n",
    "\n",
    "That's what you would normally do if you were using numpy to implement everything step by step. GradientTape is an object that stores the intermediate values and functions used to compute them, and it can be used to calculate the derivatives of any watched value. hence it can basically do all the numpy magic for you in an elegant way.\n",
    "\n",
    "GradientTape is used within a with block, upon entering the block its `__enter__` method is called which starts recording the operations which involve watched variables. when the block exits, the `__exit__` method is called which stops recording the operations and returns the `GradientTape` object. after that you can carry out **only 1** gradient operation on any watched value, then the gradient is garbage collected, if you want to do more then pass the consturctor a `persistent=True` argument."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with tf.GradientTape(persistent=True) as g:\n",
    "  x = tf.Variable(2.0)\n",
    "  y = tf.Variable(3.0)\n",
    "  z = x * y\n",
    "\n",
    "dz_dx = g.gradient(z, x) # dz/dx = y = 3\n",
    "dz_dy = g.gradient(z, y) # dz/dy = x = 2\n",
    "\n",
    "print(f\"dz/dx = {dz_dx.numpy()}\")\n",
    "print(f\"dz/dy = {dz_dy.numpy()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dz/dx = 3.0\n",
      "dz/dy = 2.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# with weights and Bias \n",
    "\n",
    "let's see how we can do that with weights and bias for a single step of training for a single layer with weights `w` and bias `b`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# prepare data\n",
    "x = np.arange(10)\n",
    "y = 2 * x - 1 # w --> 2, b --> -1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "w = tf.Variable(0.1, name=\"w\", trainable=True, dtype=tf.float32)\n",
    "b = tf.Variable(0.1, name=\"b\", trainable=True, dtype=tf.float32)\n",
    "\n",
    "def simple_loss(y_true, y_pred):\n",
    "  return tf.abs(y_true - y_pred)\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# train for 1000 epochs\n",
    "for i in range(1000):\n",
    "  with tf.GradientTape(persistent=True) as g:\n",
    "    z = w * x + b\n",
    "    loss = simple_loss(y, z)\n",
    "\n",
    "  dl_dw = g.gradient(loss, w)\n",
    "  dl_db = g.gradient(loss, b) \n",
    "\n",
    "  w.assign_sub(dl_dw * LEARNING_RATE)\n",
    "  b.assign_sub(dl_db * LEARNING_RATE)\n",
    "  del g # delete the gradient tape, must be done since we are using persistent=True\n",
    "\n",
    "print(f\"w = {w.numpy()}\") # --> 2.0\n",
    "print(f\"b = {b.numpy()}\") # --> -1.0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w = 2.00075364112854\n",
      "b = -0.9990085959434509\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now you might think that you would need to have [`w`, `b`] values for each layer, and loop over them to do the gradient calculations.\n",
    "\n",
    "But actually is gets even easier, you only need to use `model.trainable_variables` and compute the gradients for all of them in one go using \n",
    "\n",
    "```python\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "```\n",
    "\n",
    "and with that you got all the gradients. \n",
    "\n",
    "now you can apply them, will we have to do the loop now?, still NO.\n",
    "\n",
    "It's the optimizer's job that given the gradients it will update the weights, so you simply pass them to the optimizer and let it take care of it.\n",
    "\n",
    "and so your training step should look something like this:\n",
    "\n",
    "```python\n",
    "def train_step(model, data):\n",
    "    with tf.GradientTape() as tape: # not persistent since we are calling tape.gradient() only once\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "\n",
    "    loss_history.append(loss.numpy().mean())\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "```\n",
    "\n",
    "even simpler and cleaner than the code above and can train any model generally."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Higher order derivatives\n",
    "\n",
    "Simple nested gradients, check the next cell for an example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "f = lambda x: x ** 3\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  with tf.GradientTape() as g2:\n",
    "    y = f(x)\n",
    "  dy_dx = g2.gradient(y, x) # --> 3 * x ** 2 = 3 * 4 = 12\n",
    "d2y_dx2 = g.gradient(dy_dx, x) # --> 6 * x = 12\n",
    "\n",
    "print(f\"dy/dx = {dy_dx.numpy()}\")\n",
    "print(f\"d2y/dx2 = {d2y_dx2.numpy()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dy/dx = 12.0\n",
      "d2y/dx2 = 12.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('TFX': conda)"
  },
  "interpreter": {
   "hash": "8290c4fa312f0275d52774cf1cfc50565c3a57a31ddf7b44870733d485867938"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}