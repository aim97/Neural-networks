{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Model \n",
    "\n",
    "We can build models by multiple ways, initally we were using the `Sequential` model, which enables us to stack layers on top of one another, simple and easy to use and understand for beginners.  \n",
    "\n",
    "But as our needs became more complicated, we wanted to connect layers in all sorts of ways, which can't be achieved with the `Sequential` model, and so we tended to use the `Functional API`, which allows layers to have multiple inputs or multiple output.  \n",
    "\n",
    "The `Functional API` is actually perfect but we want to its components to be as simple as possible, and to do that we can extend the `Model` class, which will allow us to define reusable models that can be used to build complex models.\n",
    "\n",
    "this is done as follows:\n",
    "\n",
    "```python\n",
    "class CustomModel(Model):\n",
    "  def __init__(self, customization_args):\n",
    "    super(CustomModel, self).__init__()\n",
    "    # define the layers here\n",
    "    # for example:\n",
    "    self.dense_layer = Dense(customization_args['units'])\n",
    "\n",
    "  def call(self, inputs, training=None, mask=None):\n",
    "    # define the forward pass here (connect the layers with input here)\n",
    "    output = self.dense_layer(inputs)\n",
    "\n",
    "    return output\n",
    "\n",
    "```\n",
    "\n",
    "You need to implement at least 2 methods:\n",
    "\n",
    "1. `__init__`: This method is called when the model is created. It is used to define the layers of the model.\n",
    "2. `call`: This method is called when the model is used to make predictions. It is used to define the forward pass of the model.\n",
    "\n",
    "The advatages of using the `Model` subclass is that it would allow you to use the functionality defined by keras, like fitting, evaluation, saving and loading models.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.dense1 = tf.keras.layers.Dense(10, activation='relu')\n",
    "    self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense1(inputs)\n",
    "    x = self.dense2(x)\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will define some custom models, and demonstrate how to use them.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Identity Block\n",
    "\n",
    "Resnet depends on the idea of identity blocks, you can reed more about this in the [Resnet paper](https://arxiv.org/pdf/1512.03385.pdf).  \n",
    "\n",
    "The identity block consists of two convolutional layers, followed by a batch normalization layer and a relu activation.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "class IdentityBlock(Model):\n",
    "  def __init__(self, filters, kernel_size, **kwargs):\n",
    "    super(IdentityBlock, self).__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "    self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n",
    "    self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    self.identityAdd = tf.keras.layers.Add()\n",
    "\n",
    "    self.act = tf.keras.layers.Activation('relu') # those are like lambda layers, they are stateless and only apply their activation function\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv1(inputs)\n",
    "    x = self.bn1(x)\n",
    "    x = self.act(x)\n",
    "\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "\n",
    "    x = self.identityAdd([x, inputs])\n",
    "    x = self.act(x)\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resnet Model\n",
    "\n",
    "The model is as follows:\n",
    "\n",
    "1. Convolutional layer: `64` filters, `7x7`\n",
    "2. Batch Normalization\n",
    "3. Activation: ReLU\n",
    "4. MaxPooling: 2D `3x3`\n",
    "5. 2 Identity Blocks: `64` filters, `3x3`\n",
    "6. Global Average Pooling\n",
    "7. Dense: Classiifier layer, `num_classes` units"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ResNet(Model):\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(64, 7, padding='same')\n",
    "    self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "    self.act = tf.keras.layers.Activation('relu')\n",
    "    self.maxpool = tf.keras.layers.MaxPool2D(3)\n",
    "\n",
    "    self.identity1 = IdentityBlock(64, 3)\n",
    "    self.identity2 = IdentityBlock(64, 3)\n",
    "\n",
    "    self.avgpool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    self.fc = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.conv1(inputs)\n",
    "    x = self.bn1(x)\n",
    "    x = self.act(x)\n",
    "    x = self.maxpool(x)\n",
    "\n",
    "    x = self.identity1(x)\n",
    "    x = self.identity2(x)\n",
    "\n",
    "    x = self.avgpool(x)\n",
    "    x = self.fc(x)\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VGG-16 Model\n",
    "\n",
    "Building VGG-16 follows the same principle as Resnet, but with a different architecture.\n",
    "\n",
    "We first start by defining a recurring block, that contains few convolutional layers, followed by a max pooling layer.\n",
    "\n",
    "The end of the model is simply a flatten followed by two dense layers.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class VggBlock(Model):\n",
    "  def __init__(self, filters_count, kernel_size , strides, pool_size, cnn_count, prefix):\n",
    "    super(VggBlock, self).__init__()\n",
    "    \n",
    "    self.filters_count = filters_count\n",
    "    self.cnn_count = cnn_count\n",
    "\n",
    "    self.cnns = []\n",
    "\n",
    "    for i in range(cnn_count):\n",
    "      cnn = tf.keras.layers.Conv2D(\n",
    "        filters=filters_count,\n",
    "        kernel_size=kernel_size, \n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        name=f'{prefix}_cnn_{i}'\n",
    "      )\n",
    "      self.cnns.append(cnn)\n",
    "\n",
    "    # the strides is the stride of the pooling, not the convolution\n",
    "    self.pool = tf.keras.layers.MaxPool2D(pool_size=pool_size, strides=strides)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = inputs\n",
    "    for cnn in self.cnns:\n",
    "      x = cnn(x)\n",
    "      x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = self.pool(x)\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VGG Model\n",
    "\n",
    "it consists of the following blocks:\n",
    "\n",
    "1. VggBlock: `2` cnns, `64` filters, `3x3`\n",
    "2. VggBlock: `2` cnns, `128` filters, `3x3`\n",
    "3. VggBlock: `3` cnns, `256` filters, `3x3`\n",
    "4. VggBlock: `3` cnns, `512` filters, `3x3`\n",
    "5. VggBlock: `3` cnns, `512` filters, `3x3`\n",
    "6. Flatten layer\n",
    "7. Dense layer: `256` units, `relu` activation\n",
    "8. Dense layer: `num_classes` units, `softmax` activation\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class VGG16(Model):\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(VGG16, self).__init__()\n",
    "    self.block1 = VggBlock(filters_count=64, kernel_size=3, strides=1, pool_size=2, cnn_count=2, prefix='block1')\n",
    "    self.block2 = VggBlock(128, 3, 1, 2, 2, 'block2')\n",
    "    self.block3 = VggBlock(256, 3, 1, 2, 3, 'block3')\n",
    "    self.block4 = VggBlock(512, 3, 1, 2, 3, 'block4')\n",
    "    self.block5 = VggBlock(512, 3, 1, 2, 3, 'block5')\n",
    "\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.fc = tf.keras.layers.Dense(256, activation='relu')\n",
    "    self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.block1(inputs)\n",
    "    x = self.block2(x)\n",
    "    x = self.block3(x)\n",
    "    x = self.block4(x)\n",
    "    x = self.block5(x)\n",
    "\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc(x)\n",
    "    x = self.classifier(x)\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Danger\n",
    "\n",
    "**NOTE:** Only run this in Colab and using GPU\n",
    "\n",
    "1. It downloads a large dataset (namely Cat vs Dog dataset)\n",
    "2. Training takes a long time\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load('cats_vs_dogs', split=tfds.Split.TRAIN, data_dir='data/')\n",
    "\n",
    "# Initialize VGG with the number of classes \n",
    "vgg = VGG16(num_classes=2)\n",
    "\n",
    "# Compile with losses and metrics\n",
    "vgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess(features):\n",
    "    # Resize and normalize\n",
    "    image = tf.image.resize(features['image'], (224, 224))\n",
    "    return tf.cast(image, tf.float32) / 255., features['label']\n",
    "\n",
    "# Apply transformations to dataset\n",
    "dataset = dataset.map(preprocess).batch(32)\n",
    "\n",
    "# Train the custom VGG model\n",
    "vgg.fit(dataset, epochs=10)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('TFX': conda)"
  },
  "interpreter": {
   "hash": "8290c4fa312f0275d52774cf1cfc50565c3a57a31ddf7b44870733d485867938"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}